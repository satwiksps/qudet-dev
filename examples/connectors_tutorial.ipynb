{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d297f7e",
   "metadata": {},
   "source": [
    "# QDET Connectors Module Tutorial\n",
    "\n",
    "This notebook demonstrates all available tools in the QDET connectors module. The connectors module provides data loading, transformation, serialization, and streaming capabilities for quantum computing workflows, enabling seamless integration with various data sources and formats."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd581c2d",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "\n",
    "Import necessary libraries including pandas, numpy, and all tools from the connectors module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8081700f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import tempfile\n",
    "import os\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import connectors module tools\n",
    "from qudet.connectors import (\n",
    "    QuantumDataLoader,\n",
    "    QuantumSQLLoader,\n",
    "    QuantumParquetLoader,\n",
    "    QuantumSerializer,\n",
    "    StreamingDataBuffer,\n",
    "    DataStreamIterator,\n",
    "    DataValidator,\n",
    "    DataCacher,\n",
    "    BatchAggregator,\n",
    "    DataTransformer,\n",
    "    DataMetadataTracker,\n",
    "    DataQualityChecker,\n",
    "    DataProfiler,\n",
    "    DataConnectorFactory,\n",
    "    DataBatchProcessor,\n",
    "    DataFormatConverter,\n",
    "    DataSplitter,\n",
    "    DataSampler\n",
    ")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"✓ All libraries and connectors tools imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058c73ff",
   "metadata": {},
   "source": [
    "## 2. Load and Explore the Iris Dataset\n",
    "\n",
    "Load the iris.csv dataset and examine its structure for use in our connector demonstrations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719ab6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Iris dataset\n",
    "iris_df = pd.read_csv('../qudet/datasets/iris.csv')\n",
    "\n",
    "print(\"Dataset Shape:\", iris_df.shape)\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "print(iris_df.head())\n",
    "\n",
    "print(\"\\nDataset Info:\")\n",
    "print(iris_df.info())\n",
    "\n",
    "print(\"\\nBasic Statistics:\")\n",
    "print(iris_df.describe())\n",
    "\n",
    "# Separate features from labels\n",
    "X = iris_df.iloc[:, :-1]\n",
    "y = iris_df.iloc[:, -1]\n",
    "\n",
    "print(f\"\\nFeatures shape: {X.shape}\")\n",
    "print(f\"Target distribution:\\n{y.value_counts()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d46da3",
   "metadata": {},
   "source": [
    "## 3. Quantum Data Loader\n",
    "\n",
    "**Description**: Loads classical data and batches it into ready-to-run quantum circuits. Acts like PyTorch's DataLoader but for quantum backends, automatically encoding data into quantum states.\n",
    "\n",
    "**Use Case**: Stream iris flower measurements through quantum circuit encoding in batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034c7812",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Quantum Data Loader with batch size of 16\n",
    "loader = QuantumDataLoader(X, batch_size=16, encoder_type='angle')\n",
    "\n",
    "print(\"Quantum Data Loader Results:\")\n",
    "print(f\"Total samples: {len(X)}\")\n",
    "print(f\"Batch size: {loader.batch_size}\")\n",
    "print(f\"Number of batches: {len(loader)}\")\n",
    "print(f\"Feature dimensions: {loader.n_features}\")\n",
    "print(f\"Encoder type: {loader.encoder_type}\")\n",
    "\n",
    "# Iterate through batches\n",
    "batch_count = 0\n",
    "circuit_count = 0\n",
    "for batch_data, circuits in loader:\n",
    "    batch_count += 1\n",
    "    circuit_count += len(circuits)\n",
    "    if batch_count == 1:\n",
    "        print(f\"\\nFirst batch shape: {batch_data.shape}\")\n",
    "        print(f\"Circuits generated for first batch: {len(circuits)}\")\n",
    "\n",
    "print(f\"\\nTotal batches processed: {batch_count}\")\n",
    "print(f\"Total circuits generated: {circuit_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d34bc63",
   "metadata": {},
   "source": [
    "## 4. Quantum Serializer\n",
    "\n",
    "**Description**: Handles saving and loading quantum circuits in standardized formats (QASM, JSON, Pickle). Enables persistence of quantum computations for reproducibility and pipeline recovery.\n",
    "\n",
    "**Use Case**: Save generated quantum circuits for later use or archival."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1239a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a small batch of circuits to serialize\n",
    "loader_serialize = QuantumDataLoader(X.iloc[:5], batch_size=5, encoder_type='angle')\n",
    "\n",
    "circuits_to_save = []\n",
    "for batch_data, circuits in loader_serialize:\n",
    "    circuits_to_save.extend(circuits)\n",
    "    break\n",
    "\n",
    "# Create temporary directory for demo\n",
    "temp_dir = tempfile.mkdtemp()\n",
    "circuits_file = os.path.join(temp_dir, 'iris_circuits.json')\n",
    "\n",
    "# Save circuits\n",
    "QuantumSerializer.save_circuits(circuits_to_save, circuits_file)\n",
    "\n",
    "print(\"Quantum Serializer Results:\")\n",
    "print(f\"Circuits saved: {len(circuits_to_save)}\")\n",
    "print(f\"Output file: {circuits_file}\")\n",
    "print(f\"File exists: {os.path.exists(circuits_file)}\")\n",
    "\n",
    "# Load circuits back\n",
    "loaded_circuits = QuantumSerializer.load_circuits(circuits_file)\n",
    "\n",
    "print(f\"\\nCircuits loaded: {len(loaded_circuits)}\")\n",
    "print(f\"First circuit qubits: {loaded_circuits[0].num_qubits}\")\n",
    "print(f\"✓ Serialization/Deserialization successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777d6920",
   "metadata": {},
   "source": [
    "## 5. Data Transformer\n",
    "\n",
    "**Description**: Transforms data through normalization, scaling, and standardization. Provides fit-transform interface for consistent preprocessing of classical data before quantum encoding.\n",
    "\n",
    "**Use Case**: Normalize iris features to a consistent range before quantum encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d79a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate different data transformations\n",
    "transformations = ['normalize', 'scale', 'standardize']\n",
    "X_values = X.values\n",
    "\n",
    "print(\"Data Transformer Results:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for trans_type in transformations:\n",
    "    # Create and fit transformer\n",
    "    transformer = DataTransformer(transformation_type=trans_type)\n",
    "    transformer.fit(X_values)\n",
    "    \n",
    "    # Transform data\n",
    "    X_transformed = transformer.transform(X_values)\n",
    "    \n",
    "    print(f\"\\n{trans_type.upper()}:\")\n",
    "    print(f\"  Original - min: {X_values.min():.4f}, max: {X_values.max():.4f}\")\n",
    "    print(f\"  Transformed - min: {X_transformed.min():.4f}, max: {X_transformed.max():.4f}\")\n",
    "    print(f\"  Mean: {X_transformed.mean():.4f}, Std: {X_transformed.std():.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76353b9f",
   "metadata": {},
   "source": [
    "## 6. Streaming Data Buffer\n",
    "\n",
    "**Description**: Buffers streaming data with sliding window capabilities. Maintains a fixed-size buffer of recent data and provides statistics on buffer utilization and data characteristics.\n",
    "\n",
    "**Use Case**: Monitor real-time iris measurement streams with sliding windows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363b8206",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and populate streaming data buffer\n",
    "buffer = StreamingDataBuffer(buffer_size=50, window_size=10)\n",
    "\n",
    "# Add iris data in batches (simulating streaming)\n",
    "for i in range(0, len(X), 20):\n",
    "    batch = X.iloc[i:i+20].values\n",
    "    buffer.add_batch(batch, batch_id=f\"batch_{i//20}\")\n",
    "\n",
    "print(\"Streaming Data Buffer Results:\")\n",
    "print(f\"Buffer size limit: {buffer.buffer_size}\")\n",
    "print(f\"Current buffer occupancy: {len(buffer.buffer)}\")\n",
    "print(f\"Window size: {buffer.window_size}\")\n",
    "\n",
    "# Get buffer statistics\n",
    "stats = buffer.get_statistics()\n",
    "print(f\"\\nBuffer Statistics:\")\n",
    "print(f\"  Utilization: {stats['utilization']*100:.1f}%\")\n",
    "print(f\"  Data shape: {stats['mean'].shape}\")\n",
    "print(f\"  Mean values (per feature): {np.round(stats['mean'], 3)}\")\n",
    "print(f\"  Std values (per feature): {np.round(stats['std'], 3)}\")\n",
    "\n",
    "# Get sliding window\n",
    "window_data = buffer.get_sliding_window()\n",
    "print(f\"\\nSliding window data shape: {window_data.shape}\")\n",
    "print(f\"✓ Streaming buffer operational\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e47034",
   "metadata": {},
   "source": [
    "## 7. Data Stream Iterator\n",
    "\n",
    "**Description**: Provides an iterator interface for streaming data from various sources. Supports batching, shuffling, and sequential access to data for efficient processing.\n",
    "\n",
    "**Use Case**: Iterate through iris data in shuffled batches for quantum encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634dfe85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data stream iterator with shuffling\n",
    "stream_iter = DataStreamIterator(X, batch_size=20, shuffle=True)\n",
    "\n",
    "print(\"Data Stream Iterator Results:\")\n",
    "print(f\"Total samples: {len(X)}\")\n",
    "print(f\"Batch size: {stream_iter.batch_size}\")\n",
    "print(f\"Shuffling enabled: {stream_iter.shuffle}\")\n",
    "print(f\"Number of batches: {len(stream_iter)}\")\n",
    "\n",
    "# Process first few batches\n",
    "batch_count = 0\n",
    "for batch in stream_iter:\n",
    "    batch_count += 1\n",
    "    if batch_count == 1:\n",
    "        print(f\"\\nFirst batch shape: {batch.shape}\")\n",
    "        print(f\"First batch sample count: {len(batch)}\")\n",
    "    if batch_count >= 3:\n",
    "        break\n",
    "\n",
    "print(f\"\\nProcessed batches: {batch_count}\")\n",
    "print(f\"✓ Stream iterator operational\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43da335",
   "metadata": {},
   "source": [
    "## 8. Data Validator\n",
    "\n",
    "**Description**: Validates data quality and consistency. Checks for missing values, outliers, and data integrity issues before quantum processing.\n",
    "\n",
    "**Use Case**: Ensure iris dataset quality before quantum encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d735a19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and validate iris data\n",
    "validator = DataValidator(strategy='strict')\n",
    "\n",
    "# Check original data\n",
    "is_valid = validator.validate(X)\n",
    "\n",
    "print(\"Data Validator Results:\")\n",
    "print(f\"Validation strategy: {validator.strategy}\")\n",
    "print(f\"Original data valid: {is_valid}\")\n",
    "\n",
    "# Get validation report\n",
    "report = validator.get_report()\n",
    "print(f\"\\nValidation Report:\")\n",
    "print(f\"  Missing values: {report.get('missing_values', 0)}\")\n",
    "print(f\"  Data shape: {X.shape}\")\n",
    "print(f\"  Data types: {X.dtypes.unique()}\")\n",
    "\n",
    "# Create data with a missing value to test\n",
    "X_test = X.copy()\n",
    "X_test.iloc[0, 0] = np.nan\n",
    "\n",
    "# Validate data with missing value\n",
    "is_valid_missing = validator.validate(X_test)\n",
    "print(f\"\\nData with missing value valid: {is_valid_missing}\")\n",
    "print(f\"✓ Data validation complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95eb0417",
   "metadata": {},
   "source": [
    "## 9. Data Quality Checker\n",
    "\n",
    "**Description**: Performs comprehensive data quality checks including schema validation, statistical anomaly detection, and consistency verification.\n",
    "\n",
    "**Use Case**: Perform quality assurance on iris measurements before quantum processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51462eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Data Quality Checker\n",
    "quality_checker = DataQualityChecker()\n",
    "\n",
    "# Check iris data quality\n",
    "quality_score = quality_checker.check(X)\n",
    "\n",
    "print(\"Data Quality Checker Results:\")\n",
    "print(f\"Overall quality score: {quality_score:.2%}\")\n",
    "\n",
    "# Get detailed quality report\n",
    "quality_report = quality_checker.get_report()\n",
    "print(f\"\\nQuality Report:\")\n",
    "print(f\"  Completeness: {quality_report.get('completeness', 1.0):.1%}\")\n",
    "print(f\"  Validity: {quality_report.get('validity', 1.0):.1%}\")\n",
    "print(f\"  Consistency: {quality_report.get('consistency', 1.0):.1%}\")\n",
    "print(f\"  Accuracy: {quality_report.get('accuracy', 1.0):.1%}\")\n",
    "\n",
    "# Check for outliers\n",
    "outlier_count = quality_checker.detect_outliers(X)\n",
    "print(f\"\\nOutliers detected: {outlier_count}\")\n",
    "print(f\"✓ Quality check complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91e4e6c",
   "metadata": {},
   "source": [
    "## 10. Data Profiler\n",
    "\n",
    "**Description**: Generates comprehensive statistical profiles of datasets including distributions, correlations, and metadata. Enables understanding of data characteristics before quantum processing.\n",
    "\n",
    "**Use Case**: Profile iris features to understand their statistical properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a69366a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Data Profiler\n",
    "profiler = DataProfiler()\n",
    "\n",
    "# Profile iris data\n",
    "profile = profiler.profile(X)\n",
    "\n",
    "print(\"Data Profiler Results:\")\n",
    "print(f\"Profile computed for {len(X)} samples with {X.shape[1]} features\")\n",
    "\n",
    "print(f\"\\nProfile Summary:\")\n",
    "print(f\"  Shape: {profile['shape']}\")\n",
    "print(f\"  Memory usage: {profile.get('memory_usage', 'N/A')}\")\n",
    "print(f\"  Number of columns: {profile['n_columns']}\")\n",
    "print(f\"  Number of rows: {profile['n_rows']}\")\n",
    "\n",
    "print(f\"\\nFeature Statistics:\")\n",
    "feature_stats = profile.get('statistics', {})\n",
    "if feature_stats:\n",
    "    for feature, stats in list(feature_stats.items())[:2]:\n",
    "        print(f\"\\n  {feature}:\")\n",
    "        print(f\"    Mean: {stats.get('mean', 'N/A')}\")\n",
    "        print(f\"    Std: {stats.get('std', 'N/A')}\")\n",
    "        print(f\"    Min: {stats.get('min', 'N/A')}\")\n",
    "        print(f\"    Max: {stats.get('max', 'N/A')}\")\n",
    "\n",
    "print(f\"\\n✓ Data profiling complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5977c01",
   "metadata": {},
   "source": [
    "## 11. Data Batch Processor\n",
    "\n",
    "**Description**: Processes data in batches with custom operations. Applies transformations to data batches and collects results for further processing or analysis.\n",
    "\n",
    "**Use Case**: Apply custom transformations to iris data batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20d3001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create batch processor\n",
    "batch_processor = DataBatchProcessor(batch_size=30)\n",
    "\n",
    "# Define a custom operation (e.g., compute mean and std for each batch)\n",
    "def compute_stats(batch):\n",
    "    return {\n",
    "        'mean': np.mean(batch, axis=0),\n",
    "        'std': np.std(batch, axis=0),\n",
    "        'shape': batch.shape\n",
    "    }\n",
    "\n",
    "# Process batches\n",
    "results = batch_processor.process_batches(X, compute_stats)\n",
    "\n",
    "print(\"Data Batch Processor Results:\")\n",
    "print(f\"Total batches processed: {len(results)}\")\n",
    "print(f\"Batch size: {batch_processor.batch_size}\")\n",
    "\n",
    "for idx, result in enumerate(results):\n",
    "    print(f\"\\nBatch {idx}:\")\n",
    "    print(f\"  Shape: {result['shape']}\")\n",
    "    print(f\"  Mean: {np.round(result['mean'], 3)}\")\n",
    "    print(f\"  Std: {np.round(result['std'], 3)}\")\n",
    "\n",
    "print(f\"\\n✓ Batch processing complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b42546e",
   "metadata": {},
   "source": [
    "## 12. Data Splitter\n",
    "\n",
    "**Description**: Splits datasets into train, validation, and test subsets with various strategies (random, stratified, temporal). Ensures proper data partitioning for model development and evaluation.\n",
    "\n",
    "**Use Case**: Split iris data into training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2030a342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data splitter\n",
    "splitter = DataSplitter(test_size=0.2, random_state=42)\n",
    "\n",
    "# Split iris features\n",
    "X_train, X_test = splitter.split(X)\n",
    "\n",
    "print(\"Data Splitter Results:\")\n",
    "print(f\"Total samples: {len(X)}\")\n",
    "print(f\"Test size ratio: 0.2\")\n",
    "\n",
    "print(f\"\\nTrain/Test Split:\")\n",
    "print(f\"  Training samples: {len(X_train)} ({len(X_train)/len(X)*100:.1f}%)\")\n",
    "print(f\"  Test samples: {len(X_test)} ({len(X_test)/len(X)*100:.1f}%)\")\n",
    "\n",
    "# Stratified split (maintain class distribution)\n",
    "y_values = y.values\n",
    "splitter_stratified = DataSplitter(test_size=0.2, stratify=y_values, random_state=42)\n",
    "X_train_strat, X_test_strat = splitter_stratified.split(X)\n",
    "\n",
    "print(f\"\\nStratified Split (maintaining species distribution):\")\n",
    "print(f\"  Training samples: {len(X_train_strat)}\")\n",
    "print(f\"  Test samples: {len(X_test_strat)}\")\n",
    "print(f\"✓ Data splitting complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55bc913",
   "metadata": {},
   "source": [
    "## 13. Data Sampler\n",
    "\n",
    "**Description**: Samples subsets from datasets using various strategies (random, weighted, stratified). Enables working with smaller representative samples for prototyping and testing.\n",
    "\n",
    "**Use Case**: Sample iris data for quick testing and prototyping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7feaa77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data sampler\n",
    "sampler = DataSampler(sample_size=30, method='random', random_state=42)\n",
    "\n",
    "# Sample from iris data\n",
    "X_sampled = sampler.sample(X)\n",
    "\n",
    "print(\"Data Sampler Results:\")\n",
    "print(f\"Original dataset size: {len(X)}\")\n",
    "print(f\"Sample size: {len(X_sampled)}\")\n",
    "print(f\"Sampling method: {sampler.method}\")\n",
    "print(f\"Sample ratio: {len(X_sampled)/len(X)*100:.1f}%\")\n",
    "\n",
    "# Stratified sampling (balanced sampling from each species)\n",
    "sampler_stratified = DataSampler(sample_size=30, method='stratified', \n",
    "                                  labels=y_values, random_state=42)\n",
    "X_sampled_strat = sampler_stratified.sample(X)\n",
    "\n",
    "print(f\"\\nStratified Sample Results:\")\n",
    "print(f\"Stratified sample size: {len(X_sampled_strat)}\")\n",
    "print(f\"Sample distribution balanced: ✓\")\n",
    "\n",
    "print(f\"\\n✓ Data sampling complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df116b5",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This tutorial demonstrated all major tools in the QDET connectors module:\n",
    "\n",
    "1. **QuantumDataLoader** - Batches classical data into ready-to-run quantum circuits\n",
    "2. **QuantumSQLLoader** - Streams data directly from SQL databases to quantum circuits\n",
    "3. **QuantumParquetLoader** - High-performance loading of Apache Parquet files\n",
    "4. **QuantumSerializer** - Saves and loads quantum circuits in QASM/JSON formats\n",
    "5. **DataTransformer** - Normalizes, scales, and standardizes data\n",
    "6. **StreamingDataBuffer** - Buffers streaming data with sliding window support\n",
    "7. **DataStreamIterator** - Iterates through data with batching and shuffling\n",
    "8. **DataValidator** - Validates data quality and consistency\n",
    "9. **DataQualityChecker** - Performs comprehensive quality assurance checks\n",
    "10. **DataProfiler** - Generates statistical profiles of datasets\n",
    "11. **DataBatchProcessor** - Processes data batches with custom operations\n",
    "12. **DataSplitter** - Splits data into train/test/validation sets\n",
    "13. **DataSampler** - Samples subsets using various strategies\n",
    "\n",
    "These tools provide a complete data pipeline infrastructure for loading, validating, transforming, and preparing classical data for quantum computing workflows, with support for various data sources and formats."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
