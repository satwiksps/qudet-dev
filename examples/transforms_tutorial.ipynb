{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24e04eaa",
   "metadata": {},
   "source": [
    "# QDET Transforms Module Tutorial\n",
    "\n",
    "This notebook demonstrates all available tools in the QDET transforms module. The transforms module provides data preprocessing, dimensionality reduction, feature engineering, and normalization techniques optimized for quantum machine learning pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09873520",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "\n",
    "Import necessary libraries including pandas, numpy, matplotlib, and all tools from the transforms module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139c2508",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "# Import transforms module tools\n",
    "from qudet.transforms import (\n",
    "    QuantumNormalizer,\n",
    "    RangeNormalizer,\n",
    "    DecimalScaler,\n",
    "    LogTransformer,\n",
    "    PowerTransformer,\n",
    "    FeatureScaler,\n",
    "    FeatureSelector,\n",
    "    OutlierRemover,\n",
    "    DataBalancer,\n",
    "    CategoricalEncoder,\n",
    "    TargetEncoder,\n",
    "    FrequencyEncoder,\n",
    "    BinningEncoder,\n",
    "    QuantumPCA,\n",
    "    RandomProjector,\n",
    "    StreamingHasher,\n",
    "    QuantumImputer,\n",
    "    CoresetReducer,\n",
    "    AutoReducer\n",
    ")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"✓ All libraries and transform tools imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5887b02f",
   "metadata": {},
   "source": [
    "## 2. Load and Explore the Iris Dataset\n",
    "\n",
    "Load the iris.csv dataset and prepare it for transformation demonstrations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1692d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Iris dataset\n",
    "iris_df = pd.read_csv('../qudet/datasets/iris.csv')\n",
    "\n",
    "# Separate features and labels\n",
    "X = iris_df.iloc[:, :-1]\n",
    "y = iris_df.iloc[:, -1]\n",
    "\n",
    "print(\"Dataset Summary:\")\n",
    "print(f\"Shape: {iris_df.shape}\")\n",
    "print(f\"\\nFirst 5 rows:\")\n",
    "print(iris_df.head())\n",
    "\n",
    "print(f\"\\nFeature Statistics:\")\n",
    "print(X.describe())\n",
    "\n",
    "print(f\"\\nTarget Distribution:\")\n",
    "print(y.value_counts())\n",
    "\n",
    "# Store for use in later demonstrations\n",
    "X_values = X.values\n",
    "y_values = y.values\n",
    "feature_names = X.columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840895ef",
   "metadata": {},
   "source": [
    "## 3. Quantum Normalizer\n",
    "\n",
    "**Description**: Normalizes features to quantum-compatible scales (L2 norm, probability, amplitude, angle). Essential for quantum algorithms that require specific input ranges.\n",
    "\n",
    "**Use Case**: Normalize iris features for quantum state preparation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060288e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different normalization methods\n",
    "print(\"Quantum Normalizer Results:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "methods = ['l2', 'l1', 'probability']\n",
    "normalizers = {}\n",
    "\n",
    "for method in methods:\n",
    "    normalizer = QuantumNormalizer(method=method)\n",
    "    normalizer.fit(X_values)\n",
    "    X_normalized = normalizer.transform(X_values)\n",
    "    normalizers[method] = X_normalized\n",
    "    \n",
    "    print(f\"\\n{method.upper()} Normalization:\")\n",
    "    print(f\"  Original range: [{X_values.min():.4f}, {X_values.max():.4f}]\")\n",
    "    print(f\"  Normalized range: [{X_normalized.min():.4f}, {X_normalized.max():.4f}]\")\n",
    "    print(f\"  First row norms: {np.linalg.norm(X_normalized[0:3], axis=1)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e415a4",
   "metadata": {},
   "source": [
    "## 4. Feature Scaler\n",
    "\n",
    "**Description**: Scales and normalizes features using multiple strategies (standard, min-max, robust, quantum). Prepares data for quantum algorithms with specific input requirements.\n",
    "\n",
    "**Use Case**: Scale iris features to consistent ranges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2c338b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different scaling methods\n",
    "print(\"Feature Scaler Results:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "scaling_methods = ['standard', 'minmax', 'robust']\n",
    "\n",
    "for method in scaling_methods:\n",
    "    scaler = FeatureScaler(method=method, feature_range=(0, 1))\n",
    "    scaler.fit(X_values)\n",
    "    X_scaled = scaler.transform(X_values)\n",
    "    \n",
    "    print(f\"\\n{method.upper()} Scaling:\")\n",
    "    print(f\"  Range: [{X_scaled.min():.4f}, {X_scaled.max():.4f}]\")\n",
    "    print(f\"  Mean: {X_scaled.mean():.4f}, Std: {X_scaled.std():.4f}\")\n",
    "    print(f\"  First sample (scaled): {np.round(X_scaled[0], 3)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"✓ Feature scaling complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4a734d",
   "metadata": {},
   "source": [
    "## 5. Feature Selector\n",
    "\n",
    "**Description**: Selects the most relevant features for quantum algorithms using statistical methods. Reduces feature dimensionality while preserving predictive power.\n",
    "\n",
    "**Use Case**: Select important iris features for quantum classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c5418a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature selection\n",
    "selector = FeatureSelector(k=2, method='f_classif')\n",
    "selector.fit(X_values, y_values)\n",
    "X_selected = selector.transform(X_values)\n",
    "\n",
    "print(\"Feature Selector Results:\")\n",
    "print(f\"Original features: {len(feature_names)}\")\n",
    "print(f\"Selected features: {X_selected.shape[1]}\")\n",
    "\n",
    "# Get feature importance scores\n",
    "importance = selector.get_feature_importance()\n",
    "selected_indices = selector.get_selected_features()\n",
    "\n",
    "print(f\"\\nSelected feature indices: {selected_indices}\")\n",
    "print(f\"Selected feature names: {[feature_names[i] for i in selected_indices]}\")\n",
    "\n",
    "print(f\"\\nFeature importance scores:\")\n",
    "for idx, score in enumerate(importance):\n",
    "    print(f\"  {feature_names[idx]}: {score:.4f}\")\n",
    "\n",
    "print(f\"\\nDimensionality reduction: {len(feature_names)} → {X_selected.shape[1]} features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85b356e",
   "metadata": {},
   "source": [
    "## 6. Outlier Remover\n",
    "\n",
    "**Description**: Detects and removes outliers from datasets using statistical methods (IQR, Z-score, isolation forest). Essential for data quality before quantum processing.\n",
    "\n",
    "**Use Case**: Clean iris data by removing statistical outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d720bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qudet.transforms.feature_engineering import OutlierRemover\n",
    "\n",
    "# Create outlier remover with IQR method\n",
    "outlier_remover = OutlierRemover(method='iqr', threshold=1.5)\n",
    "\n",
    "# Fit and transform data\n",
    "X_cleaned = outlier_remover.fit_transform(X_train)\n",
    "\n",
    "# Display outlier statistics\n",
    "print(f\"Original dataset shape: {X_train.shape}\")\n",
    "print(f\"Cleaned dataset shape: {X_cleaned.shape}\")\n",
    "print(f\"Number of outliers removed: {X_train.shape[0] - X_cleaned.shape[0]}\")\n",
    "print(f\"Percentage of data removed: {(X_train.shape[0] - X_cleaned.shape[0]) / X_train.shape[0] * 100:.2f}%\")\n",
    "\n",
    "# Show outlier detection with Z-score method\n",
    "outlier_remover_zscore = OutlierRemover(method='zscore', threshold=3)\n",
    "X_zscore_cleaned = outlier_remover_zscore.fit_transform(X_train)\n",
    "print(f\"\\nZ-score method: Removed {X_train.shape[0] - X_zscore_cleaned.shape[0]} outliers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a915a9bf",
   "metadata": {},
   "source": [
    "## 7. Data Balancer\n",
    "\n",
    "**Description**: Balances class distribution in imbalanced datasets using techniques like oversampling, undersampling, and SMOTE. Ensures fair quantum circuit training on all classes.\n",
    "\n",
    "**Use Case**: Balance iris dataset for better minority class representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4acff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qudet.transforms.feature_engineering import DataBalancer\n",
    "from collections import Counter\n",
    "\n",
    "# Original class distribution\n",
    "original_counts = Counter(y_train)\n",
    "print(\"Original class distribution:\")\n",
    "print(f\"  {original_counts}\")\n",
    "\n",
    "# Balance dataset using SMOTE\n",
    "balancer = DataBalancer(strategy='smote', random_state=42)\n",
    "X_balanced, y_balanced = balancer.fit_transform(X_train, y_train)\n",
    "\n",
    "balanced_counts = Counter(y_balanced)\n",
    "print(f\"\\nBalanced class distribution:\")\n",
    "print(f\"  {balanced_counts}\")\n",
    "print(f\"\\nOriginal dataset size: {len(y_train)}\")\n",
    "print(f\"Balanced dataset size: {len(y_balanced)}\")\n",
    "\n",
    "# Try random oversampling\n",
    "balancer_oversample = DataBalancer(strategy='oversample', random_state=42)\n",
    "X_over, y_over = balancer_oversample.fit_transform(X_train, y_train)\n",
    "over_counts = Counter(y_over)\n",
    "print(f\"\\nOversampling result: {over_counts}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3fd7cef",
   "metadata": {},
   "source": [
    "## 8. Categorical Encoder\n",
    "\n",
    "**Description**: Encodes categorical features using label, one-hot, ordinal, or binary encoding. Prepares categorical data for quantum circuits.\n",
    "\n",
    "**Use Case**: Encode iris species labels using different encoding schemes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2907fd20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qudet.transforms.encoding import CategoricalEncoder\n",
    "import numpy as np\n",
    "\n",
    "# Create categorical data (iris species)\n",
    "categories = np.array(['Setosa', 'Versicolor', 'Virginica', 'Setosa', 'Versicolor'])\n",
    "\n",
    "# Label encoding\n",
    "label_encoder = CategoricalEncoder(encoding='label')\n",
    "encoded_label = label_encoder.fit_transform(categories.reshape(-1, 1))\n",
    "print(\"Label Encoding:\")\n",
    "print(f\"  Original: {categories}\")\n",
    "print(f\"  Encoded: {encoded_label.flatten()}\")\n",
    "\n",
    "# One-hot encoding\n",
    "onehot_encoder = CategoricalEncoder(encoding='onehot')\n",
    "encoded_onehot = onehot_encoder.fit_transform(categories.reshape(-1, 1))\n",
    "print(f\"\\nOne-Hot Encoding:\")\n",
    "print(f\"  Shape: {encoded_onehot.shape}\")\n",
    "print(f\"  Sample:\\n{encoded_onehot[:2]}\")\n",
    "\n",
    "# Ordinal encoding\n",
    "ordinal_encoder = CategoricalEncoder(encoding='ordinal')\n",
    "encoded_ordinal = ordinal_encoder.fit_transform(categories.reshape(-1, 1))\n",
    "print(f\"\\nOrdinal Encoding:\")\n",
    "print(f\"  Encoded: {encoded_ordinal.flatten()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5196315",
   "metadata": {},
   "source": [
    "## 9. Target Encoder\n",
    "\n",
    "**Description**: Encodes categorical features based on target variable statistics. Useful for encoding features with target correlation information.\n",
    "\n",
    "**Use Case**: Encode iris species using target-based statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1588ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qudet.transforms.encoding import TargetEncoder\n",
    "\n",
    "# Create categorical feature and target\n",
    "categories = pd.DataFrame({'species': ['Setosa', 'Versicolor', 'Virginica', 'Setosa', 'Versicolor']})\n",
    "target = np.array([0, 1, 2, 0, 1])\n",
    "\n",
    "# Target encoding\n",
    "target_encoder = TargetEncoder()\n",
    "encoded_target = target_encoder.fit_transform(categories, target)\n",
    "print(\"Target Encoding:\")\n",
    "print(f\"  Original categories: {categories['species'].values}\")\n",
    "print(f\"  Target values: {target}\")\n",
    "print(f\"  Encoded values:\\n{encoded_target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a7115b",
   "metadata": {},
   "source": [
    "## 10. Frequency Encoder\n",
    "\n",
    "**Description**: Encodes categorical features based on their frequency in the dataset. Maps categories to their occurrence counts.\n",
    "\n",
    "**Use Case**: Frequency-based encoding of iris species."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6152827c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qudet.transforms.encoding import FrequencyEncoder\n",
    "\n",
    "# Create categorical feature with different frequencies\n",
    "categories = np.array(['Setosa', 'Versicolor', 'Virginica', 'Setosa', 'Versicolor', 'Setosa']).reshape(-1, 1)\n",
    "\n",
    "# Frequency encoding\n",
    "freq_encoder = FrequencyEncoder()\n",
    "encoded_freq = freq_encoder.fit_transform(categories)\n",
    "print(\"Frequency Encoding:\")\n",
    "print(f\"  Original: {categories.flatten()}\")\n",
    "print(f\"  Frequency encoded:\\n{encoded_freq.flatten()}\")\n",
    "print(f\"  (Setosa appears 3 times = 0.5, Versicolor 2 times = 0.333, Virginica 1 time = 0.167)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e1ab85",
   "metadata": {},
   "source": [
    "## 11. Binning Encoder\n",
    "\n",
    "**Description**: Bins continuous features into discrete categories or encodes categorical features into bins. Reduces feature dimensionality and creates interpretable intervals.\n",
    "\n",
    "**Use Case**: Bin continuous iris features into discrete categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104440f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qudet.transforms.encoding import BinningEncoder\n",
    "\n",
    "# Bin continuous features into discrete categories\n",
    "binning_encoder = BinningEncoder(n_bins=3, strategy='quantile')\n",
    "X_binned = binning_encoder.fit_transform(X_train)\n",
    "print(\"Binning Encoder:\")\n",
    "print(f\"  Original feature ranges:\")\n",
    "print(f\"    Min: {X_train.min(axis=0)}\")\n",
    "print(f\"    Max: {X_train.max(axis=0)}\")\n",
    "print(f\"\\n  Binned data (first 5 samples):\")\n",
    "print(f\"    Original:\\n{X_train[:5]}\")\n",
    "print(f\"    Binned:\\n{X_binned[:5]}\")\n",
    "print(f\"\\n  Unique bin values per feature: {[len(np.unique(X_binned[:, i])) for i in range(X_binned.shape[1])]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c86a90",
   "metadata": {},
   "source": [
    "## 12. Quantum PCA\n",
    "\n",
    "**Description**: Performs quantum-inspired Principal Component Analysis using kernel methods. Efficiently reduces dimensionality while preserving quantum-relevant features.\n",
    "\n",
    "**Use Case**: Reduce 4D iris features to 2D using quantum PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a1849a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qudet.transforms.pca import QuantumPCA\n",
    "\n",
    "# Apply quantum PCA\n",
    "qpca = QuantumPCA(n_components=2, kernel='rbf')\n",
    "X_qpca = qpca.fit_transform(X_train)\n",
    "print(\"Quantum PCA Results:\")\n",
    "print(f\"  Original shape: {X_train.shape}\")\n",
    "print(f\"  Reduced shape: {X_qpca.shape}\")\n",
    "print(f\"  Explained variance ratio: {qpca.explained_variance_ratio_ if hasattr(qpca, 'explained_variance_ratio_') else 'N/A'}\")\n",
    "print(f\"\\n  First 5 samples (original vs PCA):\")\n",
    "print(f\"    Original:\\n{X_train[:5]}\")\n",
    "print(f\"    Quantum PCA:\\n{X_qpca[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e445dd",
   "metadata": {},
   "source": [
    "## 13. Random Projector\n",
    "\n",
    "**Description**: Uses Gaussian random projection for fast dimensionality reduction. Preserves distances between points while drastically reducing dimensions.\n",
    "\n",
    "**Use Case**: Project iris data to 2D using random projection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39741d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qudet.transforms.projections import RandomProjector\n",
    "\n",
    "# Apply random projection\n",
    "random_proj = RandomProjector(n_components=2, random_state=42)\n",
    "X_rp = random_proj.fit_transform(X_train)\n",
    "print(\"Random Projection Results:\")\n",
    "print(f\"  Original shape: {X_train.shape}\")\n",
    "print(f\"  Projected shape: {X_rp.shape}\")\n",
    "print(f\"  Johnson-Lindenstrauss bound: {random_proj.eps if hasattr(random_proj, 'eps') else 'N/A'}\")\n",
    "print(f\"\\n  First 5 projected samples:\")\n",
    "print(X_rp[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a433fbe",
   "metadata": {},
   "source": [
    "## 14. Streaming Hasher\n",
    "\n",
    "**Description**: Implements the hashing trick for feature vectorization. Maps arbitrary features to fixed-width vectors using hash functions. Useful for streaming data.\n",
    "\n",
    "**Use Case**: Hash categorical features to fixed-width vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b2507a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qudet.transforms.sketching import StreamingHasher\n",
    "\n",
    "# Create categorical data\n",
    "categories = pd.DataFrame({\n",
    "    'species': ['Setosa', 'Versicolor', 'Virginica', 'Setosa'],\n",
    "    'region': ['North', 'South', 'East', 'West']\n",
    "})\n",
    "\n",
    "# Hash categorical features\n",
    "hasher = StreamingHasher(n_features=8, hash_func='md5')\n",
    "X_hashed = hasher.fit_transform(categories)\n",
    "print(\"Streaming Hasher Results:\")\n",
    "print(f\"  Input shape: {categories.shape}\")\n",
    "print(f\"  Output shape (fixed-width): {X_hashed.shape}\")\n",
    "print(f\"  Hashed features (first 2 samples):\")\n",
    "print(X_hashed[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e33fb56",
   "metadata": {},
   "source": [
    "## 15. Quantum Imputer\n",
    "\n",
    "**Description**: Handles missing values using quantum similarity metrics. Imputes missing values by finding quantum-closest neighbors and interpolating their values.\n",
    "\n",
    "**Use Case**: Impute missing iris values using quantum similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81fb641",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qudet.transforms.imputation import QuantumImputer\n",
    "\n",
    "# Create data with missing values\n",
    "X_with_missing = X_train.copy()\n",
    "X_with_missing[0, 0] = np.nan\n",
    "X_with_missing[5, 2] = np.nan\n",
    "X_with_missing[10, 1] = np.nan\n",
    "\n",
    "print(\"Quantum Imputer Results:\")\n",
    "print(f\"  Missing values before imputation: {np.isnan(X_with_missing).sum()}\")\n",
    "\n",
    "# Impute using quantum similarity\n",
    "imputer = QuantumImputer(method='quantum_similarity', n_neighbors=5)\n",
    "X_imputed = imputer.fit_transform(X_with_missing)\n",
    "\n",
    "print(f\"  Missing values after imputation: {np.isnan(X_imputed).sum()}\")\n",
    "print(f\"\\n  Imputed values:\")\n",
    "print(f\"    Position [0, 0]: {X_imputed[0, 0]:.4f}\")\n",
    "print(f\"    Position [5, 2]: {X_imputed[5, 2]:.4f}\")\n",
    "print(f\"    Position [10, 1]: {X_imputed[10, 1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40d5a5c",
   "metadata": {},
   "source": [
    "## 16. Coreset Reducer\n",
    "\n",
    "**Description**: Reduces dataset size using coreset algorithms. Selects representative points that preserve dataset statistics while reducing computational complexity.\n",
    "\n",
    "**Use Case**: Reduce iris dataset to a representative coreset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe76dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qudet.transforms.coresets import CoresetReducer\n",
    "\n",
    "# Create coreset reducer\n",
    "coreset = CoresetReducer(coreset_size=30, method='kmeans')\n",
    "X_coreset = coreset.fit_transform(X_train)\n",
    "\n",
    "print(\"Coreset Reducer Results:\")\n",
    "print(f\"  Original dataset size: {X_train.shape[0]}\")\n",
    "print(f\"  Coreset size: {X_coreset.shape[0]}\")\n",
    "print(f\"  Compression ratio: {X_train.shape[0] / X_coreset.shape[0]:.2f}x\")\n",
    "print(f\"\\n  Coreset statistics:\")\n",
    "print(f\"    Mean original: {X_train.mean(axis=0)}\")\n",
    "print(f\"    Mean coreset: {X_coreset.mean(axis=0)}\")\n",
    "print(f\"    Std original: {X_train.std(axis=0)}\")\n",
    "print(f\"    Std coreset: {X_coreset.std(axis=0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98c3bec",
   "metadata": {},
   "source": [
    "## 17. Auto Reducer\n",
    "\n",
    "**Description**: Automatically selects the best dimensionality reduction technique based on data characteristics. Compares multiple methods and chooses optimal approach.\n",
    "\n",
    "**Use Case**: Automatically reduce iris dimensions using best-fit method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36b1d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qudet.transforms.auto import AutoReducer\n",
    "\n",
    "# Auto select best dimensionality reduction\n",
    "auto_reducer = AutoReducer(n_components=2, methods=['pca', 'random_projection'])\n",
    "X_auto = auto_reducer.fit_transform(X_train)\n",
    "\n",
    "print(\"Auto Reducer Results:\")\n",
    "print(f\"  Original shape: {X_train.shape}\")\n",
    "print(f\"  Reduced shape: {X_auto.shape}\")\n",
    "print(f\"  Selected method: {auto_reducer.best_method_ if hasattr(auto_reducer, 'best_method_') else 'N/A'}\")\n",
    "print(f\"  Method scores: {auto_reducer.scores_ if hasattr(auto_reducer, 'scores_') else 'N/A'}\")\n",
    "print(f\"\\n  First 5 auto-reduced samples:\")\n",
    "print(X_auto[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc15573",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated the comprehensive feature transformation capabilities of QDET's Transforms module:\n",
    "\n",
    "### Data Quality & Engineering\n",
    "- **Quantum Normalizer**: Normalizes data to quantum-compatible ranges\n",
    "- **Feature Scaler**: Applies various scaling methods (standard, minmax, robust)\n",
    "- **Feature Selector**: Identifies and selects most important features\n",
    "- **Outlier Remover**: Detects and removes statistical outliers\n",
    "- **Data Balancer**: Handles class imbalance through oversampling/undersampling\n",
    "\n",
    "### Categorical Encoding\n",
    "- **Categorical Encoder**: Maps categorical → numerical (label, onehot, ordinal)\n",
    "- **Target Encoder**: Encodes based on target variable statistics\n",
    "- **Frequency Encoder**: Encodes by feature occurrence frequency\n",
    "- **Binning Encoder**: Creates discrete intervals from continuous features\n",
    "\n",
    "### Dimensionality Reduction\n",
    "- **Quantum PCA**: Kernel-based principal component analysis\n",
    "- **Random Projector**: Gaussian random projection for fast reduction\n",
    "- **Coreset Reducer**: Selects representative points for efficiency\n",
    "- **Auto Reducer**: Automatically chooses optimal reduction method\n",
    "\n",
    "### Specialized Techniques\n",
    "- **Streaming Hasher**: Fixed-width feature hashing for streaming data\n",
    "- **Quantum Imputer**: Quantum-similarity based missing value imputation\n",
    "\n",
    "### Key Insights\n",
    "1. **Modularity**: Each transformer can be used independently or in pipelines\n",
    "2. **Compatibility**: All tools work with numpy arrays and pandas DataFrames\n",
    "3. **Quantum-Ready**: Features normalized for quantum circuit processing\n",
    "4. **Efficiency**: Scalable methods like CoresetReducer for large datasets\n",
    "5. **Flexibility**: Multiple encoding/reduction strategies for different use cases\n",
    "\n",
    "### Next Steps\n",
    "- Combine multiple transformers in pipelines for complete data preprocessing\n",
    "- Use with QDET analytics tools (classifiers, encoders) for quantum ML workflows\n",
    "- Experiment with different hyperparameters based on your specific data"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
